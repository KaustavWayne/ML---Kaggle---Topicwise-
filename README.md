Here are 50 Kaggle dataset links, categorized by the topics in your syllabus, to help you practice and apply machine learning concepts.
Linear, Multiple & Polynomial Regression (Weeks 23-25)
These datasets are great for predicting continuous values. They start simple and increase in complexity, making them perfect for practicing linear, multiple, and polynomial regression, as well as understanding regression metrics.
 * Medical Cost Personal Datasets: Predict individual medical costs billed by health insurance.
 * USA Housing: Predict house prices based on various features of the house and area.
 * Student Performance Data Set: Predict student grades based on demographic, social, and school-related features.
 * Car Price Prediction: Predict the price of used cars based on their specifications.
 * Fish Market: Predict the weight of fish based on species and physical measurements.
 * Combined Cycle Power Plant: Predict the net hourly electrical energy output of a power plant.
 * Boston Housing Dataset: A classic dataset for regression tasks, predicting median home values in Boston suburbs.
Feature Selection & Regularization (Weeks 26-27)
These datasets contain many features, making them ideal for practicing feature selection techniques (Filter, Wrapper, Embedded) and applying regularization (Ridge, Lasso, ElasticNet) to prevent overfitting.
 * Mobile Price Classification: Predict the price range of mobile phones using 20 features.
 * Santander Customer Satisfaction: Identify dissatisfied customers using hundreds of anonymized features.
 * BNP Paribas Cardif Claims Management: Predict which claims require accelerated processing using a large, anonymized dataset.
 * Ames Housing Dataset: A modern alternative to the Boston Housing dataset with 80+ features, great for feature engineering and selection.
Logistic Regression & SVM (Weeks 32-33)
These datasets are suited for binary and multi-class classification tasks. They are perfect for implementing Logistic Regression and Support Vector Machines (SVMs), including the use of different kernels.
 * Titanic - Machine Learning from Disaster: The classic beginner's competition to predict passenger survival.
 * Heart Disease UCI: Predict the presence of heart disease in a patient.
 * Customer Churn Prediction 2020: Predict whether a telecom customer will churn.
 * Breast Cancer Wisconsin (Diagnostic): Diagnose breast cancer as malignant or benign based on cell nuclei features.
 * Pima Indians Diabetes Database: Predict the onset of diabetes based on diagnostic measures.
 * Default of Credit Card Clients Dataset: Predict credit card payment default.
 * Voice Gender: Identify a voice as male or female based on acoustic properties.
K-Nearest Neighbors (KNN) & Classification Metrics (Week 28)
Use these datasets to implement the KNN algorithm for both classification and regression, and to practice evaluating models with metrics like precision, recall, and F1-score.
 * Iris Species: A simple, classic multi-class classification dataset to classify iris flower species.
 * Social Network Ads: Predict whether a user will purchase a product based on age and salary.
 * Fruit Recognition: Classify different types of fruits from images.
 * Glass Identification Data Set: Classify types of glass based on their chemical properties.
Naive Bayes & Text Classification (Week 31)
These datasets are perfect for practicing Naive Bayes, especially on textual data. They're great for building spam filters and sentiment analyzers.
 * SMS Spam Collection Dataset: Classify SMS messages as spam or ham (not spam).
 * IMDB Dataset of 50K Movie Reviews: A classic binary sentiment classification task.
 * Email Spam Classification Dataset: Another great dataset for building an email spam filter.
 * Twitter US Airline Sentiment: Analyze how travelers in February 2015 felt about major U.S. airlines.
 * News Headlines for Sarcasm Detection: Classify news headlines as sarcastic or not.
Decision Trees & Ensemble Methods (Weeks 34-36)
Explore the power of tree-based models with these datasets. They are ideal for building Decision Trees, Bagging classifiers, Random Forests, and Boosting models like XGBoost.
 * Credit Card Fraud Detection: A highly imbalanced dataset to predict fraudulent credit card transactions.
 * Bike Sharing Demand: Forecast the demand for public bikes in a city.
 * HR Analytics: Job Change of Data Scientists: Predict which data scientists are looking for a job change.
 * Mushroom Classification: A safe way to learn to identify poisonous mushrooms.
 * House Prices - Advanced Regression Techniques: A comprehensive regression challenge perfect for XGBoost.
 * Store Sales - Time Series Forecasting: Use time-series data to forecast sales, an excellent use case for boosting models.
 * TalkingData AdTracking Fraud Detection: A large-scale dataset to detect fraudulent ad clicks.
PCA & Dimensionality Reduction (Week 29)
These high-dimensional datasets are perfect for applying Principal Component Analysis (PCA) and other dimensionality reduction techniques to improve model performance and for visualization.
 * Digit Recognizer (MNIST): The "hello world" of image recognition, classify handwritten digits.
 * Fashion-MNIST: A more challenging alternative to MNIST for image classification.
 * Human Activity Recognition with Smartphones: Classify human activities using smartphone sensor data with over 500 features.
 * Olivetti Faces: A dataset of facial images, ideal for practicing PCA for face recognition.
 * CIFAR-10 - Object Recognition in Images: A more complex image dataset with 10 classes of objects.
Clustering (K-Means, DBSCAN, etc.)
For these datasets, the goal is not to predict a label but to discover natural groupings in the data. They are perfect for applying K-Means, DBSCAN, and Hierarchical Clustering.
 * Mall Customer Segmentation Data: A simple dataset to segment customers based on their spending habits.
 * Wholesale customers data set: Segment clients of a wholesale distributor.
 * Unsupervised Learning on Country Data: Categorize countries using socio-economic and health factors.
 * FIFA 22 complete player dataset: Cluster football players based on their skills and attributes.
 * Credit Card Dataset for Clustering: Segment credit card holders based on their usage patterns.
Capstone & Feature Engineering Practice
These datasets are messy, complex, and require significant feature engineering, data cleaning, and a combination of many of the techniques learned throughout the course. They are perfect for end-to-end projects.
 * Black Friday Sales Prediction: A dataset with a mix of data types and missing values, great for preprocessing practice.
 * New York City Airbnb Open Data: A rich dataset for EDA, feature engineering, and price prediction.
 * Zomato Bangalore Restaurants: A messy, real-world dataset requiring extensive cleaning and feature extraction.
 * Melbourne Housing Market: Predict housing prices with a dataset that has many columns and missing values.
 * S&P 500 Stocks (daily updated): A financial dataset that requires handling time-series data and creating relevant features.
 * Avocado Prices: A fun dataset for exploring time-series, regression, and feature engineering.
